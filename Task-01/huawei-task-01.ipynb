{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huawei-task-01.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyvMyjBXFGtt"
      },
      "outputs": [],
      "source": [
        "! pip install mindinsight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from mindspore import context\n",
        "import requests\n",
        "import mindspore.dataset as ds\n",
        "import mindspore.dataset.transforms.c_transforms as C\n",
        "import mindspore.dataset.vision.c_transforms as CV\n",
        "from mindspore.dataset.vision import Inter\n",
        "from mindspore import dtype as mstype\n",
        "import mindspore.nn as nn\n",
        "from mindspore.common.initializer import Normal\n",
        "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
        "from mindspore.nn import Accuracy\n",
        "from mindspore.train.callback import LossMonitor\n",
        "from mindspore import Model\n",
        "import numpy as np\n",
        "from mindspore import Tensor\n",
        "from mindspore import load_checkpoint, load_param_into_net\n",
        "from mindspore.train.callback import SummaryCollector\n",
        "import mindspore.ops as ops\n",
        "from mindspore.profiler import Profiler\n",
        "\n",
        "def download_dataset(dataset_url, path):\n",
        "    filename = dataset_url.split(\"/\")[-1]\n",
        "    save_path = os.path.join(path, filename)\n",
        "    if os.path.exists(save_path):\n",
        "        return\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    res = requests.get(dataset_url, stream=True, verify=False)\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        for chunk in res.iter_content(chunk_size=512):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
        "                   num_parallel_workers=1):\n",
        "    # Define the dataset.\n",
        "    mnist_ds = ds.MnistDataset(data_path)\n",
        "    resize_height, resize_width = 32, 32\n",
        "    rescale = 1.0 / 255.0\n",
        "    shift = 0.0\n",
        "    rescale_nml = 1 / 0.3081\n",
        "    shift_nml = -1 * 0.1307 / 0.3081\n",
        "\n",
        "    # Define the mapping to be operated.\n",
        "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
        "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
        "    rescale_op = CV.Rescale(rescale, shift)\n",
        "    hwc2chw_op = CV.HWC2CHW()\n",
        "    type_cast_op = C.TypeCast(mstype.int32)\n",
        "\n",
        "    # Use the map function to apply data operations to the dataset.\n",
        "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
        "    mnist_ds = mnist_ds.map(operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op], input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
        "\n",
        "\n",
        "    # Perform shuffle, batch and repeat operations.\n",
        "    buffer_size = 10000\n",
        "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
        "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
        "    mnist_ds = mnist_ds.repeat(count=repeat_size)\n",
        "\n",
        "    return mnist_ds\n",
        "\n",
        "\n",
        "class LeNet5(nn.Cell):\n",
        "    \"\"\"\n",
        "    Lenet network structure\n",
        "    \"\"\"\n",
        "    def __init__(self, num_class=10, num_channel=1):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Define the required operation.\n",
        "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
        "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
        "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
        "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.tensor_summary = ops.TensorSummary()\n",
        "\n",
        "        self.conv_summary = ops.TensorSummary()\n",
        "        # Init ImageSummary\n",
        "        self.image_summary = ops.ImageSummary()\n",
        "\n",
        "    def construct(self, x):\n",
        "        # Use the defined operation to construct a forward network.\n",
        "        self.image_summary(\"Image\", x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.conv2(x)\n",
        "        self.conv_summary(\"Convolution\", x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        self.tensor_summary(\"Tensor\", x)\n",
        "        return x\n",
        "\n",
        "def train_net(model, epoch_size, data_path, repeat_size, ckpoint_cb, summary_collector, sink_mode):\n",
        "    \"\"\"Define a training method.\"\"\"\n",
        "    # Load the training dataset.\n",
        "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
        "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125), summary_collector], dataset_sink_mode=sink_mode)\n",
        "\n",
        "def test_net(model, data_path):\n",
        "    \"\"\"Define a validation method.\"\"\"\n",
        "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
        "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
        "    print(\"{}\".format(acc))\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
        "    parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "    context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)\n",
        "\n",
        "    profiler = Profiler(output_path='./profiler_data')\n",
        "\n",
        "    train_path = \"datasets/MNIST_Data/train\"\n",
        "    test_path = \"datasets/MNIST_Data/test\"\n",
        "\n",
        "    download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\",\n",
        "                     train_path)\n",
        "    download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\",\n",
        "                     train_path)\n",
        "    download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\",\n",
        "                     test_path)\n",
        "    download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\",\n",
        "                     test_path)\n",
        "\n",
        "    net = LeNet5()\n",
        "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "    net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "    config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=20)\n",
        "    # Use model saving parameters.\n",
        "    ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
        "\n",
        "    train_epoch = 3\n",
        "    mnist_path = \"./datasets/MNIST_Data\"\n",
        "    dataset_size = 1\n",
        "    model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
        "    summary_collector = SummaryCollector(summary_dir='./summary_dir', collect_freq=1)\n",
        "    train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, summary_collector, False)\n",
        "\n",
        "    profiler.analyse()\n",
        "    test_net(model, mnist_path)\n",
        "\n",
        "    param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
        "    # Load parameters to the network.\n",
        "    load_param_into_net(net, param_dict)\n",
        "\n",
        "    ds_test = create_dataset(os.path.join(mnist_path, \"test\"), batch_size=1).create_dict_iterator()\n",
        "    data = next(ds_test)\n",
        "\n",
        "    # `images` indicates the test image, and `labels` indicates the actual classification of the test image.\n",
        "    images = data[\"image\"].asnumpy()\n",
        "    labels = data[\"label\"].asnumpy()\n",
        "\n",
        "    # Use the model.predict function to predict the classification of the image.\n",
        "    output = model.predict(Tensor(data['image']))\n",
        "    predicted = np.argmax(output.asnumpy(), axis=1)\n",
        "\n",
        "    # Output the predicted classification and the actual classification.\n",
        "    print(f'Predicted: \"{predicted[0]}\", Actual: \"{labels[0]}\"')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71jvubiUFJ4h",
        "outputId": "f19d6edc-a682-45d5-aab7-ebf0267c580b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 step: 125, loss is 2.3065526485443115\n",
            "epoch: 1 step: 250, loss is 2.3161518573760986\n",
            "epoch: 1 step: 375, loss is 2.297208070755005\n",
            "epoch: 1 step: 500, loss is 2.301285743713379\n",
            "epoch: 1 step: 625, loss is 2.326754331588745\n",
            "epoch: 1 step: 750, loss is 2.286972999572754\n",
            "epoch: 1 step: 875, loss is 2.3009390830993652\n",
            "epoch: 1 step: 1000, loss is 2.2658631801605225\n",
            "epoch: 1 step: 1125, loss is 0.5843868851661682\n",
            "epoch: 1 step: 1250, loss is 0.2988366186618805\n",
            "epoch: 1 step: 1375, loss is 0.19200237095355988\n",
            "epoch: 1 step: 1500, loss is 0.23531019687652588\n",
            "epoch: 1 step: 1625, loss is 0.3255139887332916\n",
            "epoch: 1 step: 1750, loss is 0.13904906809329987\n",
            "epoch: 1 step: 1875, loss is 0.21354234218597412\n",
            "epoch: 2 step: 125, loss is 0.028242364525794983\n",
            "epoch: 2 step: 250, loss is 0.01845042034983635\n",
            "epoch: 2 step: 375, loss is 0.06367482244968414\n",
            "epoch: 2 step: 500, loss is 0.12854845821857452\n",
            "epoch: 2 step: 625, loss is 0.25142163038253784\n",
            "epoch: 2 step: 750, loss is 0.38308781385421753\n",
            "epoch: 2 step: 875, loss is 0.1528834104537964\n",
            "epoch: 2 step: 1000, loss is 0.013235820457339287\n",
            "epoch: 2 step: 1125, loss is 0.07167413085699081\n",
            "epoch: 2 step: 1250, loss is 0.06949033588171005\n",
            "epoch: 2 step: 1375, loss is 0.02003300003707409\n",
            "epoch: 2 step: 1500, loss is 0.08902265876531601\n",
            "epoch: 2 step: 1625, loss is 0.04729603976011276\n",
            "epoch: 2 step: 1750, loss is 0.04570085555315018\n",
            "epoch: 2 step: 1875, loss is 0.015052923001348972\n",
            "epoch: 3 step: 125, loss is 0.25072017312049866\n",
            "epoch: 3 step: 250, loss is 0.005658179987221956\n",
            "epoch: 3 step: 375, loss is 0.03935491293668747\n",
            "epoch: 3 step: 500, loss is 0.1445416957139969\n",
            "epoch: 3 step: 625, loss is 0.006040920503437519\n",
            "epoch: 3 step: 750, loss is 0.008382740430533886\n",
            "epoch: 3 step: 875, loss is 0.008799384348094463\n",
            "epoch: 3 step: 1000, loss is 0.07324247062206268\n",
            "epoch: 3 step: 1125, loss is 0.05953856557607651\n",
            "epoch: 3 step: 1250, loss is 0.3608991801738739\n",
            "epoch: 3 step: 1375, loss is 0.016426146030426025\n",
            "epoch: 3 step: 1500, loss is 0.006711597554385662\n",
            "epoch: 3 step: 1625, loss is 0.015378403477370739\n",
            "epoch: 3 step: 1750, loss is 0.050018955022096634\n",
            "epoch: 3 step: 1875, loss is 0.1340230256319046\n",
            "{'Accuracy': 0.9814703525641025}\n",
            "Predicted: \"9\", Actual: \"9\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "awwJp4pEFKRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
